{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pymedtermino\n",
    "\n",
    "from metric_learn import NCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='example.log', level=logging.DEBUG)\n",
    "logging.info('START')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = 2\n",
    "population_size = 50\n",
    "max_eval_time_mins = 2\n",
    "n_jobs = 10\n",
    "max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/meddra_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_huge = pd.read_csv('../data/interim/terms_and_codes.csv')\n",
    "df_huge = df_huge[['term', 'code']].rename(columns={'term': 'text', 'code': 'meddra'})\n",
    "df_huge['meddra'] = df_huge['meddra'].apply(lambda x: int(x) if x.isdigit() else None)\n",
    "df_huge = df_huge[df_huge['meddra'].isin(df['meddra'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/interim/simple_data.csv', index=False)\n",
    "# df_huge.to_csv('../data/interim/rich_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure meddra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2153, 7), (2153,), (539, 7), (539,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/meddra_data.csv')\n",
    "meddra_labels = {v:k for k, v in enumerate(df['meddra'].unique())}\n",
    "df['meddra_label'] = df['meddra'].apply(lambda x: int(meddra_labels[x]))\n",
    "train, test = train_test_split(df, test_size=0.20)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "train['text_tokenized'] = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(train['text']), maxlen=7).tolist()\n",
    "test['text_tokenized'] = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test['text']), maxlen=7).tolist()\n",
    "train['counts'] = train['meddra_label'].apply(lambda x: train[train['meddra_label']==x].shape[0])\n",
    "#train = train[train['counts']>50]\n",
    "\n",
    "X_train = np.array(train['text_tokenized'].to_list())\n",
    "y_train = np.array(train['meddra_label'].to_list())\n",
    "\n",
    "X_test = np.array(test['text_tokenized'].to_list())\n",
    "y_test = np.array(test['meddra_label'].to_list())\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pure_data = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "with open('../data/processed/pure_data.pkl', 'wb') as data_file:\n",
    "    pickle.dump(pure_data, data_file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6cc3cdb1ac4460b4e39db8ba9e950c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/150 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a0988cf9d221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                       n_jobs=n_jobs)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'PURE MEDDRA: {score}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    861\u001b[0m                     \u001b[0;31m# raise the exception if it's our last attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    852\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0;31m# Delete the temporary cache before exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0;31m# need raise RuntimeError because no pipeline has been optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             raise RuntimeError(\n\u001b[0;32m--> 962\u001b[0;31m                 \u001b[0;34m\"A pipeline has not yet been optimized. Please call fit() first.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m             )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=generations, \n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'PURE MEDDRA: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure data with NCA (metric learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=generations, \n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "clf = make_pipeline(NCA(max_iter=max_iter), tpot)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'PURE MEDDRA with NCA: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure vectorized data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/meddra_data_simple_vec.csv')\n",
    "meddra_labels = {v:k for k, v in enumerate(df['meddra'].unique())}\n",
    "df['meddra_label'] = df['meddra'].apply(lambda x: int(meddra_labels[x]))\n",
    "df = df.drop(columns=['text', 'meddra'])\n",
    "train, test = train_test_split(df, test_size=0.20)\n",
    "\n",
    "\n",
    "X_train = np.array(train[[col for col in df.columns if col != 'meddra_label']])\n",
    "y_train = np.array(train['meddra_label'].to_list())\n",
    "\n",
    "X_test = np.array(test[[col for col in df.columns if col != 'meddra_label']])\n",
    "y_test = np.array(test['meddra_label'].to_list())\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=generations, \n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'PURE MEDDRA vectorized: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure vec data with NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=generations, \n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "\n",
    "clf = make_pipeline(NCA(max_iter=max_iter), tpot)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'PURE MEDDRA vectorized with NCA: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriched train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/rich_data.csv')\n",
    "meddra_labels = {v:k for k, v in enumerate(df['meddra'].unique())}\n",
    "df['meddra_label'] = df['meddra'].apply(lambda x: int(meddra_labels[x]))\n",
    "train, test = train_test_split(df, test_size=0.20)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "train['text_tokenized'] = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(train['text']), maxlen=7).tolist()\n",
    "test['text_tokenized'] = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test['text']), maxlen=7).tolist()\n",
    "train['counts'] = train['meddra_label'].apply(lambda x: train[train['meddra_label']==x].shape[0])\n",
    "#train = train[train['counts']>50]\n",
    "\n",
    "X_train = np.array(train['text_tokenized'].to_list())\n",
    "y_train = np.array(train['meddra_label'].to_list())\n",
    "\n",
    "X_test = np.array(test['text_tokenized'].to_list())\n",
    "y_test = np.array(test['meddra_label'].to_list())\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=generations,\n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "tpot.fit(X_train, y_train)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'ENRICHED MEDDRA: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich data with NCA (metric learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=generations, \n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "clf = make_pipeline(NCA(max_iter=max_iter), tpot)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'ENRICHED MEDDRA with NCA: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich data vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/meddra_data_rich_vec.csv')\n",
    "meddra_labels = {v:k for k, v in enumerate(df['meddra'].unique())}\n",
    "df['meddra_label'] = df['meddra'].apply(lambda x: int(meddra_labels[x]))\n",
    "train, test = train_test_split(df, test_size=0.20)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "train['text_tokenized'] = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(train['text']), maxlen=7).tolist()\n",
    "test['text_tokenized'] = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test['text']), maxlen=7).tolist()\n",
    "train['counts'] = train['meddra_label'].apply(lambda x: train[train['meddra_label']==x].shape[0])\n",
    "#train = train[train['counts']>50]\n",
    "\n",
    "X_train = np.array(train['text_tokenized'].to_list())\n",
    "y_train = np.array(train['meddra_label'].to_list())\n",
    "\n",
    "X_test = np.array(test['text_tokenized'].to_list())\n",
    "y_test = np.array(test['meddra_label'].to_list())\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=generations, \n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "tpot.fit(X_train, y_train)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'ENRICHED MEDDRA vectorized: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich data vec with NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=generations, \n",
    "                      population_size=population_size,\n",
    "                      verbosity=2, \n",
    "                      random_state=42, \n",
    "                      max_eval_time_mins=max_eval_time_mins, \n",
    "                      n_jobs=n_jobs)\n",
    "clf = make_pipeline(NCA(max_iter=max_iter), tpot)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = tpot.score(X_test, y_test)\n",
    "logging.info(f'ENRICHED MEDDRA vectorized with NCA: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
