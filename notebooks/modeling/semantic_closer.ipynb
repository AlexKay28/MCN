{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE DATA TO AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/interim/smm4h21/train.csv')\n",
    "test = pd.read_csv('../../data/interim/smm4h21/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaigorodov/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sentences = pd.read_csv('../../data/interim/cadec/test.csv')['text'].apply(\n",
    "                lambda x: x.lower().split('<SENT>')).explode().apply(lambda x: list(tokenize(x))).to_list()\n",
    "\n",
    "max_seq_len = 10\n",
    "embed_dim = 300\n",
    "nb_words = 3000\n",
    "\n",
    "ftmodel = FastText.load_fasttext_format('../../data/external/embeddings/cc.en.300.bin')\n",
    "ftmodel.train(sentences=sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>STR</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allergies</td>\n",
       "      <td>Drug allergy</td>\n",
       "      <td>10013661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HURT YOUR Liver</td>\n",
       "      <td>Liver damage</td>\n",
       "      <td>10024668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AD</td>\n",
       "      <td>Attention deficit disorder</td>\n",
       "      <td>10003731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>focus</td>\n",
       "      <td>Attention impaired</td>\n",
       "      <td>10003738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>died</td>\n",
       "      <td>Death</td>\n",
       "      <td>10011906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>orgasm</td>\n",
       "      <td>Inability to orgasm</td>\n",
       "      <td>10021574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>never have another orgasm</td>\n",
       "      <td>Inability to orgasm</td>\n",
       "      <td>10021574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>coma</td>\n",
       "      <td>Somnolence</td>\n",
       "      <td>10041349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>gain so much weight</td>\n",
       "      <td>Wilms tumour</td>\n",
       "      <td>10047986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>increase my weight</td>\n",
       "      <td>Weight increase</td>\n",
       "      <td>10047898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1712 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           term                         STR      code\n",
       "0                     allergies                Drug allergy  10013661\n",
       "1               HURT YOUR Liver                Liver damage  10024668\n",
       "2                            AD  Attention deficit disorder  10003731\n",
       "3                         focus          Attention impaired  10003738\n",
       "4                          died                       Death  10011906\n",
       "...                         ...                         ...       ...\n",
       "1707                     orgasm         Inability to orgasm  10021574\n",
       "1708  never have another orgasm         Inability to orgasm  10021574\n",
       "1709                       coma                  Somnolence  10041349\n",
       "1710        gain so much weight                Wilms tumour  10047986\n",
       "1711         increase my weight             Weight increase  10047898\n",
       "\n",
       "[1712 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['term', 'STR', 'code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaigorodov/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=nb_words,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', char_level=False\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = min(nb_words, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = ftmodel[word]\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "word_seq_train = tokenizer.texts_to_sequences(train['term'])\n",
    "word_seq_test = tokenizer.texts_to_sequences(test['term'])\n",
    "\n",
    "X_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "X_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "\n",
    "labels = pd.read_csv('../../data/interim/meddra_codes_terms_synonims.csv')\n",
    "labels = labels['CODE']\n",
    "meddra_labels = {v:k for k, v in enumerate(labels.unique())}\n",
    "\n",
    "y_train = train['code'].apply(lambda x: meddra_labels[x]).to_numpy()\n",
    "y_test = test['code'].apply(lambda x: meddra_labels[x]).to_numpy()\n",
    "\n",
    "number_of_classes = labels.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../../models/encoder/tokenizer.pickle', 'wb') as tok_bin:\n",
    "    pickle.dump(tokenizer, tok_bin, pickle.HIGHEST_PROTOCOL)\n",
    "with open('../../models/encoder/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3520, 10), (881, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 300)           900000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               51400     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 10, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 256)           467968    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10, 256)           65792     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 3000)          771000    \n",
      "=================================================================\n",
      "Total params: 3,528,224\n",
      "Trainable params: 2,628,224\n",
      "Non-trainable params: 900,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, Model\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "\n",
    "vocab_size = nb_words\n",
    "src_txt_length = max_seq_len\n",
    "sum_txt_length = max_seq_len\n",
    "\n",
    "# encoder input model\n",
    "inputs =   layers.Input(shape=(src_txt_length,))\n",
    "encoder1 = layers.Embedding(nb_words, embed_dim, input_length=max_seq_len, \n",
    "                        weights=[embedding_matrix],trainable=False)(inputs)\n",
    "encoder2 = layers.Bidirectional(layers.LSTM(256))(encoder1)\n",
    "encoder3 = layers.Dense(256, activation='relu')(encoder2)\n",
    "encoder4 = layers.Dense(200, activation='relu')(encoder3)\n",
    "encoder5 = layers.RepeatVector(sum_txt_length)(encoder4)\n",
    "\n",
    "# decoder output model\n",
    "decoder1 = layers.LSTM(256, return_sequences=True)(encoder5)\n",
    "decoder2 = layers.Dense(256, activation='relu')(decoder1)\n",
    "outputs =  layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(decoder2)\n",
    "\n",
    "# tie it together\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "110/110 [==============================] - 9s 49ms/step - loss: 3.4394 - val_loss: 1.9827\n",
      "Epoch 2/500\n",
      "110/110 [==============================] - 5s 45ms/step - loss: 1.9427 - val_loss: 1.9754\n",
      "Epoch 3/500\n",
      "110/110 [==============================] - 5s 43ms/step - loss: 1.9802 - val_loss: 1.9690\n",
      "Epoch 4/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.9430 - val_loss: 1.9610\n",
      "Epoch 5/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.9253 - val_loss: 1.9582\n",
      "Epoch 6/500\n",
      "110/110 [==============================] - 4s 39ms/step - loss: 1.9319 - val_loss: 1.9447\n",
      "Epoch 7/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8901 - val_loss: 1.9315\n",
      "Epoch 8/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8853 - val_loss: 1.9232\n",
      "Epoch 9/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8638 - val_loss: 1.9316\n",
      "Epoch 10/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8638 - val_loss: 1.9174\n",
      "Epoch 11/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8909 - val_loss: 1.9113\n",
      "Epoch 12/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8415 - val_loss: 1.9223\n",
      "Epoch 13/500\n",
      "110/110 [==============================] - 5s 43ms/step - loss: 1.8410 - val_loss: 1.9132\n",
      "Epoch 14/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8458 - val_loss: 1.9007\n",
      "Epoch 15/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.8589 - val_loss: 1.9041\n",
      "Epoch 16/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.8832 - val_loss: 1.8969\n",
      "Epoch 17/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8412 - val_loss: 1.8967\n",
      "Epoch 18/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8384 - val_loss: 1.8929\n",
      "Epoch 19/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8356 - val_loss: 1.8949\n",
      "Epoch 20/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.7989 - val_loss: 1.8926\n",
      "Epoch 21/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8521 - val_loss: 1.8891\n",
      "Epoch 22/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8823 - val_loss: 1.8924\n",
      "Epoch 23/500\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 1.8288 - val_loss: 1.9055\n",
      "Epoch 24/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8373 - val_loss: 1.9185\n",
      "Epoch 25/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8194 - val_loss: 1.8893\n",
      "Epoch 26/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.8507 - val_loss: 1.8980\n",
      "Epoch 27/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8434 - val_loss: 1.8856\n",
      "Epoch 28/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.8486 - val_loss: 1.8914\n",
      "Epoch 29/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8215 - val_loss: 1.8852\n",
      "Epoch 30/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8311 - val_loss: 1.9104\n",
      "Epoch 31/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.8420 - val_loss: 1.8847\n",
      "Epoch 32/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8313 - val_loss: 1.8851\n",
      "Epoch 33/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8066 - val_loss: 1.9025\n",
      "Epoch 34/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8177 - val_loss: 1.8843\n",
      "Epoch 35/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.8451 - val_loss: 1.8850\n",
      "Epoch 36/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.8222 - val_loss: 1.8839\n",
      "Epoch 37/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.8596 - val_loss: 1.8424\n",
      "Epoch 38/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.7256 - val_loss: 1.4828\n",
      "Epoch 39/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.4215 - val_loss: 1.4072\n",
      "Epoch 40/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.3262 - val_loss: 1.3490\n",
      "Epoch 41/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.3046 - val_loss: 1.3275\n",
      "Epoch 42/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.2363 - val_loss: 1.3126\n",
      "Epoch 43/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.2314 - val_loss: 1.2742\n",
      "Epoch 44/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 1.1990 - val_loss: 1.2331\n",
      "Epoch 45/500\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 1.1570 - val_loss: 1.2024\n",
      "Epoch 46/500\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 1.0959 - val_loss: 1.1688\n",
      "Epoch 47/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 1.0897 - val_loss: 1.1338\n",
      "Epoch 48/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 1.0297 - val_loss: 1.1133\n",
      "Epoch 49/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.9876 - val_loss: 1.0981\n",
      "Epoch 50/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.9788 - val_loss: 1.0824\n",
      "Epoch 51/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.9617 - val_loss: 1.0754\n",
      "Epoch 52/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.9265 - val_loss: 1.0680\n",
      "Epoch 53/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.8912 - val_loss: 1.0363\n",
      "Epoch 54/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.8898 - val_loss: 1.0409\n",
      "Epoch 55/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.8542 - val_loss: 1.0251\n",
      "Epoch 56/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.8436 - val_loss: 1.0145\n",
      "Epoch 57/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.8162 - val_loss: 1.0162\n",
      "Epoch 58/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.8247 - val_loss: 1.0064\n",
      "Epoch 59/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.8072 - val_loss: 0.9677\n",
      "Epoch 60/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.7694 - val_loss: 0.9593\n",
      "Epoch 61/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.7531 - val_loss: 0.9543\n",
      "Epoch 62/500\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 0.7314 - val_loss: 0.9591\n",
      "Epoch 63/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.7328 - val_loss: 0.9459\n",
      "Epoch 64/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.7356 - val_loss: 0.9284\n",
      "Epoch 65/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.7043 - val_loss: 0.9288\n",
      "Epoch 66/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.6872 - val_loss: 0.9153\n",
      "Epoch 67/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.6752 - val_loss: 0.9164\n",
      "Epoch 68/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.6648 - val_loss: 0.9129\n",
      "Epoch 69/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.6555 - val_loss: 0.8963\n",
      "Epoch 70/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.6251 - val_loss: 0.8986\n",
      "Epoch 71/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.6061 - val_loss: 0.8861\n",
      "Epoch 72/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.6088 - val_loss: 0.9053\n",
      "Epoch 73/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.6122 - val_loss: 0.8781\n",
      "Epoch 74/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.5948 - val_loss: 0.9051\n",
      "Epoch 75/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.6004 - val_loss: 0.8757\n",
      "Epoch 76/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.5541 - val_loss: 0.8825\n",
      "Epoch 77/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.5518 - val_loss: 0.8652\n",
      "Epoch 78/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.5376 - val_loss: 0.8633\n",
      "Epoch 79/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.5181 - val_loss: 0.8711\n",
      "Epoch 80/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.5214 - val_loss: 0.8599\n",
      "Epoch 81/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.5165 - val_loss: 0.8519\n",
      "Epoch 82/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.5053 - val_loss: 0.8668\n",
      "Epoch 83/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.4908 - val_loss: 0.8607\n",
      "Epoch 84/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.4827 - val_loss: 0.8551\n",
      "Epoch 85/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.5006 - val_loss: 0.8821\n",
      "Epoch 86/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.4707 - val_loss: 0.8498\n",
      "Epoch 87/500\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 0.4680 - val_loss: 0.8523\n",
      "Epoch 88/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.4952 - val_loss: 0.8416\n",
      "Epoch 89/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.4589 - val_loss: 0.8390\n",
      "Epoch 90/500\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 0.4469 - val_loss: 0.8378\n",
      "Epoch 91/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.4320 - val_loss: 0.8430\n",
      "Epoch 92/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.4136 - val_loss: 0.8529\n",
      "Epoch 93/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.4292 - val_loss: 0.8490\n",
      "Epoch 94/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.4295 - val_loss: 0.8376\n",
      "Epoch 95/500\n",
      "110/110 [==============================] - 5s 43ms/step - loss: 0.4083 - val_loss: 0.8488\n",
      "Epoch 96/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.3915 - val_loss: 0.8448\n",
      "Epoch 97/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3997 - val_loss: 0.8538\n",
      "Epoch 98/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.4068 - val_loss: 0.8420\n",
      "Epoch 99/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3816 - val_loss: 0.8424\n",
      "Epoch 100/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3812 - val_loss: 0.8380\n",
      "Epoch 101/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3561 - val_loss: 0.8588\n",
      "Epoch 102/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.3866 - val_loss: 0.8373\n",
      "Epoch 103/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3402 - val_loss: 0.8436\n",
      "Epoch 104/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3429 - val_loss: 0.8461\n",
      "Epoch 105/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.3571 - val_loss: 0.8469\n",
      "Epoch 106/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3342 - val_loss: 0.8626\n",
      "Epoch 107/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.3646 - val_loss: 0.8351\n",
      "Epoch 108/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3253 - val_loss: 0.8504\n",
      "Epoch 109/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.3233 - val_loss: 0.8644\n",
      "Epoch 110/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.3276 - val_loss: 0.8402\n",
      "Epoch 111/500\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 0.3325 - val_loss: 0.8528\n",
      "Epoch 112/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.3278 - val_loss: 0.8708\n",
      "Epoch 113/500\n",
      "110/110 [==============================] - 4s 40ms/step - loss: 0.2966 - val_loss: 0.8547\n",
      "Epoch 114/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.3206 - val_loss: 0.8744\n",
      "Epoch 115/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.3204 - val_loss: 0.8497\n",
      "Epoch 116/500\n",
      "110/110 [==============================] - 4s 41ms/step - loss: 0.2994 - val_loss: 0.8578\n",
      "Epoch 117/500\n",
      "110/110 [==============================] - 5s 41ms/step - loss: 0.2805 - val_loss: 0.8487\n"
     ]
    }
   ],
   "source": [
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    to_categorical(X_train, num_classes=nb_words),\n",
    "    epochs=500,\n",
    "    validation_data=(X_test, to_categorical(X_test, num_classes=nb_words)),\n",
    "    callbacks=[earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5bc819dcc0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i felt pain in my left leg'],\n",
       " array([[  0,   0,   0,   2, 104,  11,  10,   6, 105,  95]], dtype=int32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['I felt pain in my left leg']\n",
    "tokenized = tokenizer.texts_to_sequences(text)\n",
    "tokenized = sequence.pad_sequences(tokenized, maxlen=max_seq_len)\n",
    "tokenizer.sequences_to_texts(tokenized), tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['had of the of of my legs']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = [np.argmax(i) for i in model.predict(tokenized)[0]]\n",
    "tokenizer.sequences_to_texts([predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 20, 8, 4, 8, 8, 6, 69]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5ad03caae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = keras.Sequential(model.layers[:5])\n",
    "encoder.predict(tokenized).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 300)           900000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               51400     \n",
      "=================================================================\n",
      "Total params: 2,223,464\n",
      "Trainable params: 1,323,464\n",
      "Non-trainable params: 900,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('../../models/encoder/encoder_cadec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder_loaded = keras.models.load_model('../../models/encoder/encoder_cadec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5a7b01b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_loaded.predict(tokenized).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sick of this pain my back yeahh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is second message</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is third message</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0\n",
       "0  sick of this pain my back yeahh\n",
       "1           this is second message\n",
       "2            this is third message"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.DataFrame([\n",
    "    'sick of this pain my back yeahh',\n",
    "    'this is second message',\n",
    "    'this is third message'\n",
    "])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5a7b01b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sick of this pain my back yeahh</td>\n",
       "      <td>[0.0, 0.0, 2.0932157, 0.0, 0.0, 0.9086473, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is second message</td>\n",
       "      <td>[0.0, 0.0, 1.5793325, 0.0, 0.0, 0.0, 2.0198689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is third message</td>\n",
       "      <td>[0.0, 0.0, 1.5667237, 0.0, 0.0, 0.0, 2.2308083...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  \\\n",
       "0  sick of this pain my back yeahh   \n",
       "1           this is second message   \n",
       "2            this is third message   \n",
       "\n",
       "                                                 vec  \n",
       "0  [0.0, 0.0, 2.0932157, 0.0, 0.0, 0.9086473, 0.3...  \n",
       "1  [0.0, 0.0, 1.5793325, 0.0, 0.0, 0.0, 2.0198689...  \n",
       "2  [0.0, 0.0, 1.5667237, 0.0, 0.0, 0.0, 2.2308083...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized = tokenizer.texts_to_sequences(text[0])\n",
    "tokenized = sequence.pad_sequences(tokenized, maxlen=max_seq_len)\n",
    "#pd.DataFrame()\n",
    "text['vec'] = tokenizer.texts_to_sequences(text[0])\n",
    "#text['vec'] = text['vec'].apply(lambda toks: encoder_loaded.predict(toks).shape)\n",
    "text['vec'] = sequence.pad_sequences(text['vec'], maxlen=max_seq_len).tolist()\n",
    "text['vec'] = text['vec'].apply(lambda seq: encoder_loaded.predict([seq])[0])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['vec'].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
