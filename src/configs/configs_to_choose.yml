general:
  n_jobs: 25
  random_seed: 42

preprocessing:

  tokenizer_name:
    #- spacy
    - nltk
    #- gensim

  sentence_vectorizer:
    - fasttext_facebook #
    #- bert-base-uncased
    # - bert-PubMed
    #- bertweet-base
    #- encoder
    #- tfidf
    - word2vec

  agg_type:
    - avg #
    #- max

  ask_select_novelties:
    #- True
    - False

  ft_model_path:
    - data/external/embeddings/cc.en.300.bin
    #- data/external/embeddings/BioWordVec_PubMed_MIMICIII_d200.bin

  sentence_vec:
    - 50
    - 100
    - 300

modeling:
  metric_learner_name:
    #- NCA
    #- LMNN
    #- LFDA
    - siamese
  use_metric_learning:
    - True
    #- False

  siamese_params:
    batch_size:
      #- 64
      - 128
      - 256
    lr:
      - 0.001
      - 0.0005
    epochs:
      #- 30
      #- 40
      #- 50
      - 70
      - 100
      - 250
      #- 350
    steps_per_epoch:
      #- 50
      - 35
      #- 20
      #- 10
      - 70
      - 140
      #- 250
      #- 350
    alpha:
      - 0.2
      - 0.6
      - 1.0
      - 1.2

  distance_type:
    #- euclidean
    #- cosine
    - minkowski #
    - chebyshev

  model_name:
    #- SVC
    - kNN #
    #- SGD
    #s- LGBM
