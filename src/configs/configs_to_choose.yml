general:
  n_jobs: 5
  random_seed: 42

preprocessing:

  tokenizer_name:
    - spacy
    #- nltk
    #- gensim

  sentence_vectorizer:
    #- fasttext_default_100
    # - fasttext_default_300
    # - fasttext_cadec_100
    # - fasttext_cadec_300
    - fasttext_facebook
    #- bert-base-uncased
    # - bert-PubMed
    # - bertweet-base
    #- encoder

  agg_type:
    - avg
    - max
    #- reduce

  ft_model_path:
    - data/external/embeddings/cc.en.300.bin
    #- data/external/embeddings/BioWordVec_PubMed_MIMICIII_d200.bin

  fasttext_model:
    epochs:
      - 5
      - 10
      - 15
    window:
      - 3
      - 5
      - 7

  use_facebook:
    - True
    #- False

  sentence_vec:
    - 50
    - 100
    - 300

modeling:
  metric_learner_name:
    # - NCA
    #- LMNN
    #- LFDA
    - siamese
  use_metric_learning:
    #- True
    - False

  siamese_params:
    batch_size:
      #- 128
      - 256
    lr:
      - 0.001
    epochs:
      - 30
      #- 70
      #- 100
      #- 150
      - 200
      #- 250
    steps_per_epoch:
      - 50
      #- 35
      #- 20
      #- 10
      #- 100
    alpha:
      #- 0.6
      - 1.0
      #- 1.2

  distance_type:
    #- euclidean
    #- cosine
    - minkowski
    #- chebyshev

  model_name:
    #- SVC
    - kNN
    #- SGD
